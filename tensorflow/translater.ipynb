{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Configuração"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\r\n",
    "# Tipagem\r\n",
    "import typing \r\n",
    "import string\r\n",
    "from typing import Any, Tuple, NamedTuple\r\n",
    "from string import digits\r\n",
    "\r\n",
    "# Auxiliares\r\n",
    "import pathlib\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.ticker as ticker\r\n",
    "\r\n",
    "# Core\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_text as tf_text\r\n",
    "from tensorflow.keras.layers.experimental import preprocessing\r\n",
    "\r\n",
    "# Classe auxliar\r\n",
    "class ShapeChecker():\r\n",
    "  def __init__(self):\r\n",
    "    # Keep a cache of every axis-name seen\r\n",
    "    self.shapes = {}\r\n",
    "\r\n",
    "  def __call__(self, tensor, names, broadcast=False):\r\n",
    "    if not tf.executing_eagerly():\r\n",
    "      return\r\n",
    "\r\n",
    "    if isinstance(names, str):\r\n",
    "      names = (names,)\r\n",
    "\r\n",
    "    shape = tf.shape(tensor)\r\n",
    "    rank = tf.rank(tensor)\r\n",
    "\r\n",
    "    if rank != len(names):\r\n",
    "      raise ValueError(f'Rank mismatch:\\n'\r\n",
    "                       f'    found {rank}: {shape.numpy()}\\n'\r\n",
    "                       f'    expected {len(names)}: {names}\\n')\r\n",
    "\r\n",
    "    for i, name in enumerate(names):\r\n",
    "      if isinstance(name, int):\r\n",
    "        old_dim = name\r\n",
    "      else:\r\n",
    "        old_dim = self.shapes.get(name, None)\r\n",
    "      new_dim = shape[i]\r\n",
    "\r\n",
    "      if (broadcast and new_dim == 1):\r\n",
    "        continue\r\n",
    "\r\n",
    "      if old_dim is None:\r\n",
    "        # If the axis name is new, add its length to the cache.\r\n",
    "        self.shapes[name] = new_dim\r\n",
    "        continue\r\n",
    "\r\n",
    "      if new_dim != old_dim:\r\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\r\n",
    "                         f\"    found: {new_dim}\\n\"\r\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregando os dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='C://Users/ricar/Downloads/spa-eng.zip', extract=True)\r\n",
    "path_of_file = pathlib.Path('./data/dataset.txt')\r\n",
    "\r\n",
    "# função para carregar os dados\r\n",
    "def load_data(path):\r\n",
    "  text = path.read_text(encoding='utf-8')\r\n",
    "  \r\n",
    "  #pairs vai ter várias listas, com dois elementos em cada uma: a palavra em Inglês e Espanhol\r\n",
    "  lines = text.splitlines()\r\n",
    "  pairs = [line.split('\\t') for line in lines]\r\n",
    "\r\n",
    "  input = [input for target, input in pairs]\r\n",
    "  target = [target for target, input in pairs]\r\n",
    "\r\n",
    "  return target, input\r\n",
    "\r\n",
    "target, input = load_data(path_of_file)\r\n",
    "print(f'espanhol: {input[-1]}\\n')\r\n",
    "print(f'inglês: {target[-1]}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "espanhol: Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
      "\n",
      "inglês: If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estruturando os dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "BUFFER_SIZE = len(input)\r\n",
    "BATCH_SIZE = 64\r\n",
    "\r\n",
    "#Vai separar todas as listas (input, targe) em partes menores, de acordo com o tamanho por parte (BUFFER_ZISE)\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input, target)).shuffle(BUFFER_SIZE)\r\n",
    "\r\n",
    "#Combina elementos do dataset em lotes (listas) de acordo com o tamanho (BATCH_SIZE)\r\n",
    "dataset = dataset.batch(BATCH_SIZE)\r\n",
    "\r\n",
    "for example_input_batch, example_target_batch in dataset.take(1):\r\n",
    "  print(example_input_batch[:5])\r\n",
    "  print()\r\n",
    "  print(example_target_batch[:5])\r\n",
    "  break\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[b'Cog\\xc3\\xad un taxi desde la estaci\\xc3\\xb3n hasta el hotel.'\n",
      " b'Con un grado universitario, Tom conseguir\\xc3\\xa1 un mejor empleo.'\n",
      " b'Casi todas las hojas se han ca\\xc3\\xaddo.'\n",
      " b'Tom no est\\xc3\\xa1 ayudando para nada.'\n",
      " b'Tom se est\\xc3\\xa1 ti\\xc3\\xb1endo el pelo.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'I caught a cab from the station to the hotel.'\n",
      " b'Tom will get a better job with a college degree.'\n",
      " b'Almost all the leaves have fallen.' b\"Tom isn't helping any.\"\n",
      " b'Tom is dyeing his hair.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pré processamento do texto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Aqui neste método há várias regras para processar os textos nas línguas distintas\r\n",
    "def tf_lower_and_split_punctuation(text):\r\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\r\n",
    "  text = tf.strings.lower(text)\r\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\r\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\r\n",
    "  text = tf.strings.strip(text)\r\n",
    " \r\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\r\n",
    "  return text\r\n",
    "\r\n",
    "tf_lower_and_split_punctuation('¿Todavía está en casa?').numpy().decode()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[START] ¿ todavia esta en casa ? [END]'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vetorização dos textos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "max_vocabulary_size = 5000\r\n",
    "\r\n",
    "#O adapt é usado para fazer um treinamento dos vetores em cima dos dados \r\n",
    "\r\n",
    "#camada de vetorização do input \r\n",
    "input_text_processor = preprocessing.TextVectorization(standardize=tf_lower_and_split_punctuation, max_tokens=max_vocabulary_size)\r\n",
    "input_text_processor.adapt(input)\r\n",
    "\r\n",
    "#camada de vetorização do output \r\n",
    "output_text_processor = preprocessing.TextVectorization(standardize=tf_lower_and_split_punctuation, max_tokens=max_vocabulary_size)\r\n",
    "output_text_processor.adapt(target)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Usando as camadas vetorizadas\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#As camadas têm capacidade agora de converter uma lista (ou lote) de strings em textos\r\n",
    "#E com o vocabulário, pode-se converter os tokens para textos\r\n",
    "\r\n",
    "print(f'Vocabulário da camada input: {input_text_processor.get_vocabulary()[:10]}')\r\n",
    "print(f'Vocabulário da camada output: {output_text_processor.get_vocabulary()[:10]}\\n')\r\n",
    "\r\n",
    "example_tokens = input_text_processor(example_input_batch)\r\n",
    "\r\n",
    "print(f'Exemplo de tokens: {example_tokens[:3, :10]}\\n')\r\n",
    "print(f'Tokens da primeira frase: {example_tokens[0]}\\n')\r\n",
    "\r\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\r\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\r\n",
    "first_phrase = ' '.join(tokens)\r\n",
    "\r\n",
    "print(f'Primeira frase \"destokenizada\": {tokens}\\n')\r\n",
    "print(f'Primeira frase \"destokenizada\" e formatada: {first_phrase}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulário da camada input: ['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']\n",
      "Vocabulário da camada output: ['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']\n",
      "\n",
      "Exemplo de tokens: [[   2 3481   16  832  212   11  354  143    7  476]\n",
      " [   2   27   16 4197 3361   19   10    1   16  113]\n",
      " [   2  188  229   34 1651   17  300 3183    4    3]]\n",
      "\n",
      "Tokens da primeira frase: [   2 3481   16  832  212   11  354  143    7  476    4    3    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Primeira frase \"destokenizada\": ['[START]' 'cogi' 'un' 'taxi' 'desde' 'la' 'estacion' 'hasta' 'el' 'hotel'\n",
      " '.' '[END]' '' '' '' '' '' '' '' '' '' '']\n",
      "\n",
      "Primeira frase \"destokenizada\" e formatada: [START] cogi un taxi desde la estacion hasta el hotel . [END]          \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Codificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#Variáveis \"globais\"\r\n",
    "\r\n",
    "embedding_dim = 256 #Dimensão da camada de embedding\r\n",
    "units = 1024 #"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Codificador\r\n",
    "\r\n",
    "# Considerações: o uso da classe ShapeChecker() serve para verificar os formatos dos tensores\r\n",
    "#                o codificador precisa retornar uma saída codificada e também o seu estado para que seja passado ao decodificador  \r\n",
    "\r\n",
    "# O codificador, simplificando, é uma camada da inteligência de tradução, por isso herda de Layer\r\n",
    "class Encoder(tf.keras.layers.Layer):\r\n",
    "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\r\n",
    "    super(Encoder, self).__init__()\r\n",
    "    self.enc_units = enc_units\r\n",
    "    self.input_vocab_size = input_vocab_size\r\n",
    "\r\n",
    "    #Camada de embedding para converter tokens em vetores \r\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size, embedding_dim)\r\n",
    "\r\n",
    "    #Essa é a camada que processa os vetores sequencialmente (camada RNN) \r\n",
    "    self.gru = tf.keras.layers.GRU(\r\n",
    "      self.enc_units,\r\n",
    "      return_sequences=True,\r\n",
    "      return_state=True,\r\n",
    "      recurrent_initializer='glorot_uniform'\r\n",
    "    )\r\n",
    "  \r\n",
    "  def call(self, tokens, state=None):\r\n",
    "    shape_checker = ShapeChecker()\r\n",
    "    shape_checker(tokens, ('batch', 's'))\r\n",
    "\r\n",
    "    #Camada de embedding que procura os tokens\r\n",
    "    vectors = self.embedding(tokens)\r\n",
    "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\r\n",
    "\r\n",
    "    #Camada RNN que processa a sequencia de vetores\r\n",
    "    output, state = self.gru(vectors, initial_state=state)\r\n",
    "    shape_checker(output, ('batch', 's', 'enc_units'))\r\n",
    "    shape_checker(state, ('batch', 'enc_units'))\r\n",
    "\r\n",
    "    return output, state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando o codificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#Convertendo a entrada (texto) para tokens\r\n",
    "example_tokens = input_text_processor(example_input_batch)\r\n",
    "\r\n",
    "#Tenho 64 frases dentro de \"example_input_batch\"\r\n",
    "print(f'formato do lote de entrada (lote de frases): {example_input_batch.shape}')\r\n",
    "print(f'algumas frases do lote: {example_input_batch[:2]}\\n')\r\n",
    "\r\n",
    "#Tenho 64 lista de tokens, cada lista contendo 15 tokens\r\n",
    "print(f'formato do lote de tokens de entrada (lote de tokens): {example_tokens.shape}')\r\n",
    "print(f'alguns tokens que estão sendo passados: {example_tokens[:1]}\\n')\r\n",
    "\r\n",
    "#Codificando a sequência de tokens de entrada\r\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\r\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\r\n",
    "\r\n",
    "#Eu tenho uma saída, codificada, com o seguinte shape (x, 15, 1024)\r\n",
    "#Onde x é quantidade de frases que passei, 15 é quantidade de tokens e 1024 é a dimensão de cada embedding \r\n",
    "print(f'Saída codificada, shape (batch, s, units): {example_enc_output.shape}')\r\n",
    "print(f'Estado de saída, shape (batch, units): {example_enc_state.shape}\\n')\r\n",
    "\r\n",
    "#Cada frase vai ter uma lista de tokens que foram vetorizados à embeddings (transformado em uma lista de valores de embeddings)\r\n",
    "#Então agora temos 15 listas, referente ao valor dos tokens, e cada lista tem uma lista com 1024 valores de embedding \r\n",
    "print(f'Primeira frase: {example_enc_output[0].shape}')\r\n",
    "print(f'Valor de embedding do primeiro token: {example_enc_output[0][0].shape}')\r\n",
    "\r\n",
    "#----\r\n",
    "print(f'\\nInput batch, shape (batch): {example_input_batch.shape}')\r\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\r\n",
    "print(f'Input batch tokens, shape (batch, s): {example_enc_output.shape}')\r\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "formato do lote de entrada (lote de frases): (64,)\n",
      "algumas frases do lote: [b'Cog\\xc3\\xad un taxi desde la estaci\\xc3\\xb3n hasta el hotel.'\n",
      " b'Con un grado universitario, Tom conseguir\\xc3\\xa1 un mejor empleo.']\n",
      "\n",
      "formato do lote de tokens de entrada (lote de tokens): (64, 22)\n",
      "alguns tokens que estão sendo passados: [[   2 3481   16  832  212   11  354  143    7  476    4    3    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Saída codificada, shape (batch, s, units): (64, 22, 1024)\n",
      "Estado de saída, shape (batch, units): (64, 1024)\n",
      "\n",
      "Primeira frase: (22, 1024)\n",
      "Valor de embedding do primeiro token: (1024,)\n",
      "\n",
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 22)\n",
      "Input batch tokens, shape (batch, s): (64, 22, 1024)\n",
      "Encoder state, shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Atenção  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# a cabeça da camada de atenção a ser usada aqui é a Bahdanau's additive attention. \r\n",
    "# ref: https://arxiv.org/pdf/1409.0473.pdf\r\n",
    "\r\n",
    "class BahadanauAttention(tf.keras.layers.Layer):\r\n",
    "    def __init__(self, units):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\r\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\r\n",
    "\r\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\r\n",
    "\r\n",
    "    #query vai ser algo gerado pelo decodificador\r\n",
    "    #value é a saída do codificador\r\n",
    "    #maks vai servir para excluir o padding\r\n",
    "    def call(self, query, value, mask):\r\n",
    "        shape_checker = ShapeChecker()\r\n",
    "        shape_checker(query, ('batch', 't', 'query_units')) \r\n",
    "        shape_checker(value, ('batch', 's', 'value_units')) \r\n",
    "        shape_checker(mask, ('batch', 's')) \r\n",
    "\r\n",
    "        w1_query = self.W1(query)\r\n",
    "        shape_checker(w1_query, ('batch', 't', 'attn_units'))\r\n",
    "\r\n",
    "        w2_key = self.W2(value)\r\n",
    "        shape_checker(w2_key, ('batch', 's', 'attn_units'))\r\n",
    "\r\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\r\n",
    "        value_mask = mask\r\n",
    "\r\n",
    "        context_vector, attention_weights = self.attention(\r\n",
    "            inputs = [w1_query, value, w2_key],\r\n",
    "            mask=[query_mask, value_mask],\r\n",
    "            return_attention_scores = True,\r\n",
    "        )\r\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\r\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\r\n",
    "\r\n",
    "        return context_vector, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a camada de atenção"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "attention_layer = BahadanauAttention(units)\r\n",
    "print((example_tokens != 0).shape)\r\n",
    "\r\n",
    "#Este é um exemplo de consulta que o decodificador fará\r\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\r\n",
    "\r\n",
    "#\r\n",
    "context_vector, attention_weights = attention_layer(query=example_attention_query, value=example_enc_output, mask=(example_tokens != 0))\r\n",
    "\r\n",
    "print(f'Shape do resultado do vetor de atenção: {context_vector.shape}')\r\n",
    "print(f'Shape dos pesos retornados da camada de atenção: {attention_weights.shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 22)\n",
      "Shape do resultado do vetor de atenção: (64, 2, 1024)\n",
      "Shape dos pesos retornados da camada de atenção: (64, 2, 22)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decodificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Como são diversos tensores que o decoder recebe e retorna, foi criada classes auxiliares \r\n",
    "class DecoderInput(typing.NamedTuple):\r\n",
    "    new_tokens: Any\r\n",
    "    enc_output: Any\r\n",
    "    mask: Any\r\n",
    "\r\n",
    "class DecoderOutput(typing.NamedTuple):\r\n",
    "    logits: Any\r\n",
    "    attention_weights: Any    \r\n",
    "\r\n",
    "#O decodificador vai receber a saída inteira do codificador para gerar as previsões.\r\n",
    "class Decoder(tf.keras.layers.Layer): \r\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\r\n",
    "        super(Decoder, self).__init__()\r\n",
    "\r\n",
    "        self.dec_units = dec_units\r\n",
    "        self.output_vocab_size = output_vocab_size\r\n",
    "        self.embedding_dim = embedding_dim\r\n",
    "\r\n",
    "        #1º passo: criar camada de Embedding para vetorizar os IDs de tokens\r\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size, embedding_dim)\r\n",
    "\r\n",
    "        #2º passo: a camada GRU para gerar as previsões \r\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\r\n",
    "\r\n",
    "        #3º passo: gerar a camada de atenção para melhorar a previsão. A saída da camada GRU vai servir como query para esta camada\r\n",
    "        self.attention = BahadanauAttention(self.dec_units)\r\n",
    "\r\n",
    "        #4º passo: \r\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh, use_bias=False)\r\n",
    "\r\n",
    "        #5º passo: uma camada que irá produzir previsões logísticas para cada token de saída\r\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\r\n",
    "\r\n",
    "    def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\r\n",
    "        shape_checker = ShapeChecker()\r\n",
    "        shape_checker(inputs.new_tokens, ('batch', 't'))\r\n",
    "        shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\r\n",
    "        shape_checker(inputs.mask, ('batch', 's'))\r\n",
    "        \r\n",
    "        if state is not None:\r\n",
    "          shape_checker(state, ('batch', 'dec_units'))\r\n",
    "        \r\n",
    "        #1º passo. Vetorizando os tokens\r\n",
    "        vectors = self.embedding(inputs.new_tokens)\r\n",
    "        shape_checker(vectors, ('batch', 't', 'embedding_dim'))\r\n",
    "        \r\n",
    "        #2º passo. Processa o vetor de embeddings com a camada GRU\r\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\r\n",
    "        \r\n",
    "        shape_checker(rnn_output, ('batch', 't', 'dec_units'))\r\n",
    "        shape_checker(state, ('batch', 'dec_units'))\r\n",
    "        \r\n",
    "        #3º passo. Usa a saída da camda GRU como query para a camada de atenção\r\n",
    "        context_vector, attention_weights = self.attention( query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\r\n",
    "        \r\n",
    "        shape_checker(context_vector, ('batch', 't', 'dec_units'))\r\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\r\n",
    "        \r\n",
    "        #4º passo. Eqn. (3): Join the context_vector and rnn_output [ct; ht] shape: (batch t, value_units + query_units)\r\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\r\n",
    "        \r\n",
    "        #5º passo. Eqn. (3): `at = tanh(Wc@[ct; ht])`\r\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\r\n",
    "        shape_checker(attention_vector, ('batch', 't', 'dec_units'))\r\n",
    "        \r\n",
    "        #6º passo. Gera as previsões logísticas:\r\n",
    "        logits = self.fc(attention_vector)\r\n",
    "        shape_checker(logits, ('batch', 't', 'output_vocab_size'))\r\n",
    "        \r\n",
    "        return DecoderOutput(logits, attention_weights), state\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando o decodificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Relembrando os dados\r\n",
    "\r\n",
    "print(f'frase: {example_target_batch[1]}')\r\n",
    "print(f'frase vetorizada: {output_text_processor(example_target_batch[1])}')\r\n",
    "\r\n",
    "#O decoder precisa ter o vocabulário para converter os tokens\r\n",
    "#A camada de embedding precisa ter o mesmo tamanho da camada do codificador\r\n",
    "print(f'embedding: {embedding_dim}')\r\n",
    "print(f'unidades: {units}') #Relemebrar\r\n",
    "\r\n",
    "decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\r\n",
    "\r\n",
    "example_output_tokens = output_text_processor(example_target_batch)\r\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\r\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0]) #revisar isso\r\n",
    "\r\n",
    "# Testando o decoder\r\n",
    "dec_result, dec_state = decoder(\r\n",
    "    inputs = DecoderInput(new_tokens=first_token, enc_output=example_enc_output, mask=(example_tokens != 0 )),\r\n",
    "    state = example_enc_state\r\n",
    ")\r\n",
    "\r\n",
    "print(f'\\nshape do cálculo logístico: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\r\n",
    "print(f'shape do estado do decoder: (batch_size, dec_units) {dec_state.shape}')\r\n",
    "\r\n",
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\r\n",
    "\r\n",
    "#decodificando os tokens usando o vocabulário\r\n",
    "vocab = np.array(output_text_processor.get_vocabulary())\r\n",
    "first_word = vocab[sampled_token.numpy()]\r\n",
    "first_word[:5]\r\n",
    "\r\n",
    "print(f'\\nVocabulário: {vocab}')\r\n",
    "print(f'Palavras: {first_word[:5]}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "frase: b'Tom will get a better job with a college degree.'\n",
      "frase vetorizada: [   2    9   49   66   10  192  183   36   10  960 3769    4    3]\n",
      "embedding: 256\n",
      "unidades: 1024\n",
      "\n",
      "shape do cálculo logístico: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "shape do estado do decoder: (batch_size, dec_units) (64, 1024)\n",
      "\n",
      "Vocabulário: ['' '[UNK]' '[START]' ... 'productive' 'printer' 'principles']\n",
      "Palavras: [['lot']\n",
      " ['blue']\n",
      " ['award']\n",
      " ['sells']\n",
      " ['hi']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tcc': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "f16f457e3894a9ee51386b4536b92eca290ee443d329ce528bb63897bfdf32bc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}