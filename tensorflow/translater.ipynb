{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Configuração"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "# Tipagem\r\n",
    "import typing \r\n",
    "from typing import Any, Tuple\r\n",
    "\r\n",
    "# Auxiliares\r\n",
    "import pathlib\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.ticker as ticker\r\n",
    "\r\n",
    "# Core\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_text as tf_text\r\n",
    "from tensorflow.keras.layers.experimental import preprocessing\r\n",
    "\r\n",
    "# Classe auxliar\r\n",
    "class ShapeChecker():\r\n",
    "  def __init__(self):\r\n",
    "    # Keep a cache of every axis-name seen\r\n",
    "    self.shapes = {}\r\n",
    "\r\n",
    "  def __call__(self, tensor, names, broadcast=False):\r\n",
    "    if not tf.executing_eagerly():\r\n",
    "      return\r\n",
    "\r\n",
    "    if isinstance(names, str):\r\n",
    "      names = (names,)\r\n",
    "\r\n",
    "    shape = tf.shape(tensor)\r\n",
    "    rank = tf.rank(tensor)\r\n",
    "\r\n",
    "    if rank != len(names):\r\n",
    "      raise ValueError(f'Rank mismatch:\\n'\r\n",
    "                       f'    found {rank}: {shape.numpy()}\\n'\r\n",
    "                       f'    expected {len(names)}: {names}\\n')\r\n",
    "\r\n",
    "    for i, name in enumerate(names):\r\n",
    "      if isinstance(name, int):\r\n",
    "        old_dim = name\r\n",
    "      else:\r\n",
    "        old_dim = self.shapes.get(name, None)\r\n",
    "      new_dim = shape[i]\r\n",
    "\r\n",
    "      if (broadcast and new_dim == 1):\r\n",
    "        continue\r\n",
    "\r\n",
    "      if old_dim is None:\r\n",
    "        # If the axis name is new, add its length to the cache.\r\n",
    "        self.shapes[name] = new_dim\r\n",
    "        continue\r\n",
    "\r\n",
    "      if new_dim != old_dim:\r\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\r\n",
    "                         f\"    found: {new_dim}\\n\"\r\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1560/237108084.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Auxiliares\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregando os dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='C://Users/ricar/Downloads/spa-eng.zip', extract=True)\r\n",
    "path_of_file = pathlib.Path('./data/dataset.txt')\r\n",
    "\r\n",
    "# função para carregar os dados\r\n",
    "def load_data(path):\r\n",
    "  text = path.read_text(encoding='utf-8')\r\n",
    "  \r\n",
    "  #pairs vai ter várias listas, com dois elementos em cada uma: a palavra em Inglês e Espanhol\r\n",
    "  lines = text.splitlines()\r\n",
    "  pairs = [line.split('\\t') for line in lines]\r\n",
    "\r\n",
    "  input = [input for target, input in pairs]\r\n",
    "  target = [target for target, input in pairs]\r\n",
    "\r\n",
    "  return target, input\r\n",
    "\r\n",
    "target, input = load_data(path_of_file)\r\n",
    "print(f'espanhol: {input[-1]}\\n')\r\n",
    "print(f'inglês: {target[-1]}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "espanhol: Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
      "\n",
      "inglês: If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estruturando os dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "BUFFER_SIZE = len(input)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Vai separar todas as listas (input, targe) em partes menores, de acordo com o tamanho por parte (BUFFER_ZISE)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input, target)).shuffle(BUFFER_SIZE)\n",
    "\n",
    "#Combina elementos do dataset em lotes (listas) de acordo com o tamanho (BATCH_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "  print(example_input_batch[:5])\n",
    "  print()\n",
    "  print(example_target_batch[:5])\n",
    "  break\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[b'Han mejorado los negocios.' b'Preferir\\xc3\\xada quedarme.'\n",
      " b'\\xc2\\xbfAcaso viste c\\xc3\\xb3mo me mir\\xc3\\xb3?'\n",
      " b'Se considera que una persona con un IMC de 25 a 29 padece sobrepeso.'\n",
      " b'La erupci\\xc3\\xb3n volc\\xc3\\xa1nica amenazaba a la aldea.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'Business has improved.' b\"I'd rather stay.\"\n",
      " b'Did you see how he looked at me?'\n",
      " b'A person with a BMI of 25 to 29 is considered overweight.'\n",
      " b'The volcanic eruption threatened the village.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pré processamento do texto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Aqui neste método há várias regras para processar os textos nas línguas distintas\n",
    "def tf_lower_and_split_punctuation(text):\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  text = tf.strings.strip(text)\n",
    " \n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text\n",
    "\n",
    "tf_lower_and_split_punctuation('¿Todavía está en casa?').numpy().decode()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[START] ¿ todavia esta en casa ? [END]'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vetorização dos textos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "max_vocabulary_size = 5000\n",
    "\n",
    "#O adapt é usado para fazer um treinamento dos vetores em cima dos dados \n",
    "\n",
    "#camada de vetorização do input \n",
    "input_text_processor = preprocessing.TextVectorization(standardize=tf_lower_and_split_punctuation, max_tokens=max_vocabulary_size)\n",
    "input_text_processor.adapt(input)\n",
    "\n",
    "#camada de vetorização do output \n",
    "output_text_processor = preprocessing.TextVectorization(standardize=tf_lower_and_split_punctuation, max_tokens=max_vocabulary_size)\n",
    "output_text_processor.adapt(target)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Usando as camadas vetorizadas\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#As camadas têm capacidade agora de converter uma lista (ou lote) de strings em textos\n",
    "#E com o vocabulário, pode-se converter os tokens para textos\n",
    "\n",
    "print(f'Vocabulário da camada input: {input_text_processor.get_vocabulary()[:10]}')\n",
    "print(f'Vocabulário da camada output: {output_text_processor.get_vocabulary()[:10]}\\n')\n",
    "\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "print(f'Exemplo de tokens: {example_tokens[:3, :10]}\\n')\n",
    "print(f'Tokens da primeira frase: {example_tokens[0]}\\n')\n",
    "\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "first_phrase = ' '.join(tokens)\n",
    "\n",
    "print(f'Primeira frase \"destokenizada\": {tokens}\\n')\n",
    "print(f'Primeira frase \"destokenizada\" e formatada: {first_phrase}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulário da camada input: ['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']\n",
      "Vocabulário da camada output: ['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']\n",
      "\n",
      "Exemplo de tokens: [[   2  300 4425   26 1209    4    3    0    0    0]\n",
      " [   2 1264  865    4    3    0    0    0    0    0]\n",
      " [   2   13  680  782   38   18  661   12    3    0]]\n",
      "\n",
      "Tokens da primeira frase: [   2  300 4425   26 1209    4    3    0    0    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "Primeira frase \"destokenizada\": ['[START]' 'han' 'mejorado' 'los' 'negocios' '.' '[END]' '' '' '' '' '' ''\n",
      " '' '']\n",
      "\n",
      "Primeira frase \"destokenizada\" e formatada: [START] han mejorado los negocios . [END]        \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Codificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Variáveis \"globais\"\n",
    "\n",
    "embedding_dim = 256 #Dimensão da camada de embedding\n",
    "units = 1024 #"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Codificador\n",
    "\n",
    "# Considerações: o uso da classe ShapeChecker() serve para verificar os formatos dos tensores\n",
    "#                o codificador precisa retornar uma saída codificada e também o seu estado para que seja passado ao decodificador  \n",
    "\n",
    "# O codificador, simplificando, é uma camada da inteligência de tradução, por isso herda de Layer\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.enc_units = enc_units\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "\n",
    "    #Camada de embedding para converter tokens em vetores \n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size, embedding_dim)\n",
    "\n",
    "    #Essa é a camada que processa os vetores sequencialmente (camada RNN) \n",
    "    self.gru = tf.keras.layers.GRU(\n",
    "      self.enc_units,\n",
    "      return_sequences=True,\n",
    "      return_state=True,\n",
    "      recurrent_initializer='glorot_uniform'\n",
    "    )\n",
    "  \n",
    "  def call(self, tokens, state=None):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "    #Camada de embedding que procura os tokens\n",
    "    vectors = self.embedding(tokens)\n",
    "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "    #Camada RNN que processa a sequencia de vetores\n",
    "    output, state = self.gru(vectors, initial_state=state)\n",
    "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "    shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "    return output, state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando o codificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "#Convertendo a entrada (texto) para tokens\n",
    "example_tokens = input_text_processor(example_input_batch[:32])\n",
    "\n",
    "#Tenho 64 frases dentro de \"example_input_batch\"\n",
    "print(f'formato do lote de entrada (lote de frases): {example_input_batch.shape}')\n",
    "print(f'algumas frases do lote: {example_input_batch[:2]}\\n')\n",
    "\n",
    "#Tenho 64 lista de tokens, cada lista contendo 15 tokens\n",
    "print(f'formato do lote de tokens de entrada (lote de tokens): {example_tokens.shape}')\n",
    "print(f'alguns tokens que estão sendo passados: {example_tokens[:1]}\\n')\n",
    "\n",
    "#Codificando a sequência de tokens de entrada\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "#Eu tenho uma saída, codificada, com o seguinte shape (x, 15, 1024)\n",
    "#Onde x é quantidade de frases que passei, 15 é quantidade de tokens e 1024 é a dimensão de cada embedding \n",
    "print(f'Saída codificada, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Estado de saída, shape (batch, units): {example_enc_state.shape}\\n')\n",
    "\n",
    "#Cada frase vai ter uma lista de tokens que foram vetorizados à embeddings (transformado em uma lista de valores de embeddings)\n",
    "#Então agora temos 15 listas, referente ao valor dos tokens, e cada lista tem uma lista com 1024 valores de embedding \n",
    "print(f'Primeira frase: {example_enc_output[0].shape}')\n",
    "print(f'Valor de embedding do primeiro token: {example_enc_output[0][0].shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "formato do lote de entrada (lote de frases): (64,)\n",
      "algumas frases do lote: [b'Han mejorado los negocios.' b'Preferir\\xc3\\xada quedarme.']\n",
      "\n",
      "formato do lote de tokens de entrada (lote de tokens): (32, 15)\n",
      "alguns tokens que estão sendo passados: [[   2  300 4425   26 1209    4    3    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "\n",
      "Saída codificada, shape (batch, s, units): (32, 15, 1024)\n",
      "Estado de saída, shape (batch, units): (32, 1024)\n",
      "\n",
      "Primeira frase: (15, 1024)\n",
      "Embedding da primeira frase: (1024,)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}