{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Configuração"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\r\n",
    "# Tipagem\r\n",
    "import typing \r\n",
    "import string\r\n",
    "from typing import Any, Tuple, NamedTuple\r\n",
    "from string import digits\r\n",
    "\r\n",
    "# Auxiliares\r\n",
    "import pathlib\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.ticker as ticker\r\n",
    "\r\n",
    "# Core\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_text as tf_text\r\n",
    "from tensorflow.keras.layers.experimental import preprocessing\r\n",
    "\r\n",
    "# Classe auxliar\r\n",
    "class ShapeChecker():\r\n",
    "  def __init__(self):\r\n",
    "    # Keep a cache of every axis-name seen\r\n",
    "    self.shapes = {}\r\n",
    "\r\n",
    "  def __call__(self, tensor, names, broadcast=False):\r\n",
    "    if not tf.executing_eagerly():\r\n",
    "      return\r\n",
    "\r\n",
    "    if isinstance(names, str):\r\n",
    "      names = (names,)\r\n",
    "\r\n",
    "    shape = tf.shape(tensor)\r\n",
    "    rank = tf.rank(tensor)\r\n",
    "\r\n",
    "    if rank != len(names):\r\n",
    "      raise ValueError(f'Rank mismatch:\\n'\r\n",
    "                       f'    found {rank}: {shape.numpy()}\\n'\r\n",
    "                       f'    expected {len(names)}: {names}\\n')\r\n",
    "\r\n",
    "    for i, name in enumerate(names):\r\n",
    "      if isinstance(name, int):\r\n",
    "        old_dim = name\r\n",
    "      else:\r\n",
    "        old_dim = self.shapes.get(name, None)\r\n",
    "      new_dim = shape[i]\r\n",
    "\r\n",
    "      if (broadcast and new_dim == 1):\r\n",
    "        continue\r\n",
    "\r\n",
    "      if old_dim is None:\r\n",
    "        # If the axis name is new, add its length to the cache.\r\n",
    "        self.shapes[name] = new_dim\r\n",
    "        continue\r\n",
    "\r\n",
    "      if new_dim != old_dim:\r\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\r\n",
    "                         f\"    found: {new_dim}\\n\"\r\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregando os dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='C://Users/ricar/Downloads/spa-eng.zip', extract=True)\r\n",
    "path_of_file = pathlib.Path('./data/dataset.txt')\r\n",
    "\r\n",
    "# função para carregar os dados\r\n",
    "def load_data(path):\r\n",
    "  text = path.read_text(encoding='utf-8')\r\n",
    "  \r\n",
    "  #pairs vai ter várias listas, com dois elementos em cada uma: a palavra em Inglês e Espanhol\r\n",
    "  lines = text.splitlines()\r\n",
    "  pairs = [line.split('\\t') for line in lines]\r\n",
    "\r\n",
    "  input = [input for target, input in pairs]\r\n",
    "  target = [target for target, input in pairs]\r\n",
    "\r\n",
    "  return target, input\r\n",
    "\r\n",
    "target, input = load_data(path_of_file)\r\n",
    "print(f'espanhol: {input[-1]}\\n')\r\n",
    "print(f'inglês: {target[-1]}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "espanhol: Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
      "\n",
      "inglês: If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estruturando os dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "BUFFER_SIZE = len(input)\r\n",
    "BATCH_SIZE = 64\r\n",
    "\r\n",
    "#Vai separar todas as listas (input, targe) em partes menores, de acordo com o tamanho por parte (BUFFER_ZISE)\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input, target)).shuffle(BUFFER_SIZE)\r\n",
    "\r\n",
    "#Combina elementos do dataset em lotes (listas) de acordo com o tamanho (BATCH_SIZE)\r\n",
    "dataset = dataset.batch(BATCH_SIZE)\r\n",
    "\r\n",
    "for example_input_batch, example_target_batch in dataset.take(1):\r\n",
    "  print(example_input_batch[:5])\r\n",
    "  print()\r\n",
    "  print(example_target_batch[:5])\r\n",
    "  break\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[b'Tom est\\xc3\\xa1 anonadado.'\n",
      " b'Quieres que me lave las manos primero, \\xc2\\xbfverdad?'\n",
      " b'El valor predeterminado es cero.' b'Por favor espere un momento.'\n",
      " b'No depende de sus padres.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'Tom is dumbfounded.' b\"You want me to wash my hands first, don't you?\"\n",
      " b'The default value is zero.' b'Please wait a moment.'\n",
      " b'He is independent of his parents.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pré processamento do texto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#Aqui neste método há várias regras para processar os textos nas línguas distintas\r\n",
    "def tf_lower_and_split_punctuation(text):\r\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\r\n",
    "  text = tf.strings.lower(text)\r\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\r\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\r\n",
    "  text = tf.strings.strip(text)\r\n",
    " \r\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\r\n",
    "  return text\r\n",
    "\r\n",
    "tf_lower_and_split_punctuation('¿Todavía está en casa?').numpy().decode()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[START] ¿ todavia esta en casa ? [END]'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vetorização dos textos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "max_vocabulary_size = 5000\r\n",
    "\r\n",
    "#O adapt é usado para fazer um treinamento dos vetores em cima dos dados \r\n",
    "\r\n",
    "#camada de vetorização do input \r\n",
    "input_text_processor = preprocessing.TextVectorization(standardize=tf_lower_and_split_punctuation, max_tokens=max_vocabulary_size)\r\n",
    "input_text_processor.adapt(input)\r\n",
    "\r\n",
    "#camada de vetorização do output \r\n",
    "output_text_processor = preprocessing.TextVectorization(standardize=tf_lower_and_split_punctuation, max_tokens=max_vocabulary_size)\r\n",
    "output_text_processor.adapt(target)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Usando as camadas vetorizadas\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#As camadas têm capacidade agora de converter uma lista (ou lote) de strings em textos\r\n",
    "#E com o vocabulário, pode-se converter os tokens para textos\r\n",
    "\r\n",
    "print(f'Vocabulário da camada input: {input_text_processor.get_vocabulary()[:10]}')\r\n",
    "print(f'Vocabulário da camada output: {output_text_processor.get_vocabulary()[:10]}\\n')\r\n",
    "\r\n",
    "example_tokens = input_text_processor(example_input_batch)\r\n",
    "\r\n",
    "print(f'Exemplo de tokens: {example_tokens[:3, :10]}\\n')\r\n",
    "print(f'Tokens da primeira frase: {example_tokens[0]}\\n')\r\n",
    "\r\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\r\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\r\n",
    "first_phrase = ' '.join(tokens)\r\n",
    "\r\n",
    "print(f'Primeira frase \"destokenizada\": {tokens}\\n')\r\n",
    "print(f'Primeira frase \"destokenizada\" e formatada: {first_phrase}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulário da camada input: ['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']\n",
      "Vocabulário da camada output: ['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']\n",
      "\n",
      "Exemplo de tokens: [[   2   10   20    1    4    3    0    0    0    0]\n",
      " [   2  122    5   18    1   34  510  589   19   13]\n",
      " [   2    7 1161    1   15 2800    4    3    0    0]]\n",
      "\n",
      "Tokens da primeira frase: [ 2 10 20  1  4  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "Primeira frase \"destokenizada\": ['[START]' 'tom' 'esta' '[UNK]' '.' '[END]' '' '' '' '' '' '' '' '' '' ''\n",
      " '']\n",
      "\n",
      "Primeira frase \"destokenizada\" e formatada: [START] tom esta [UNK] . [END]           \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Codificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#Variáveis \"globais\"\r\n",
    "\r\n",
    "embedding_dim = 256 #Dimensão da camada de embedding\r\n",
    "units = 1024 #"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Codificador\r\n",
    "\r\n",
    "# Considerações: o uso da classe ShapeChecker() serve para verificar os formatos dos tensores\r\n",
    "#                o codificador precisa retornar uma saída codificada e também o seu estado para que seja passado ao decodificador  \r\n",
    "\r\n",
    "# O codificador, simplificando, é uma camada da inteligência de tradução, por isso herda de Layer\r\n",
    "class Encoder(tf.keras.layers.Layer):\r\n",
    "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\r\n",
    "    super(Encoder, self).__init__()\r\n",
    "    self.enc_units = enc_units\r\n",
    "    self.input_vocab_size = input_vocab_size\r\n",
    "\r\n",
    "    #Camada de embedding para converter tokens em vetores \r\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size, embedding_dim)\r\n",
    "\r\n",
    "    #Essa é a camada que processa os vetores sequencialmente (camada RNN) \r\n",
    "    self.gru = tf.keras.layers.GRU(\r\n",
    "      self.enc_units,\r\n",
    "      return_sequences=True,\r\n",
    "      return_state=True,\r\n",
    "      recurrent_initializer='glorot_uniform'\r\n",
    "    )\r\n",
    "  \r\n",
    "  def call(self, tokens, state=None):\r\n",
    "    shape_checker = ShapeChecker()\r\n",
    "    shape_checker(tokens, ('batch', 's'))\r\n",
    "\r\n",
    "    #Camada de embedding que procura os tokens\r\n",
    "    vectors = self.embedding(tokens)\r\n",
    "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\r\n",
    "\r\n",
    "    #Camada RNN que processa a sequencia de vetores\r\n",
    "    output, state = self.gru(vectors, initial_state=state)\r\n",
    "    shape_checker(output, ('batch', 's', 'enc_units'))\r\n",
    "    shape_checker(state, ('batch', 'enc_units'))\r\n",
    "\r\n",
    "    return output, state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testando o codificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "#Convertendo a entrada (texto) para tokens\r\n",
    "example_tokens = input_text_processor(example_input_batch)\r\n",
    "\r\n",
    "#Tenho 64 frases dentro de \"example_input_batch\"\r\n",
    "print(f'formato do lote de entrada (lote de frases): {example_input_batch.shape}')\r\n",
    "print(f'algumas frases do lote: {example_input_batch[:2]}\\n')\r\n",
    "\r\n",
    "#Tenho 64 lista de tokens, cada lista contendo 15 tokens\r\n",
    "print(f'formato do lote de tokens de entrada (lote de tokens): {example_tokens.shape}')\r\n",
    "print(f'alguns tokens que estão sendo passados: {example_tokens[:1]}\\n')\r\n",
    "\r\n",
    "#Codificando a sequência de tokens de entrada\r\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\r\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\r\n",
    "\r\n",
    "#Eu tenho uma saída, codificada, com o seguinte shape (x, 15, 1024)\r\n",
    "#Onde x é quantidade de frases que passei, 15 é quantidade de tokens e 1024 é a dimensão de cada embedding \r\n",
    "print(f'Saída codificada, shape (batch, s, units): {example_enc_output.shape}')\r\n",
    "print(f'Estado de saída, shape (batch, units): {example_enc_state.shape}\\n')\r\n",
    "\r\n",
    "#Cada frase vai ter uma lista de tokens que foram vetorizados à embeddings (transformado em uma lista de valores de embeddings)\r\n",
    "#Então agora temos 15 listas, referente ao valor dos tokens, e cada lista tem uma lista com 1024 valores de embedding \r\n",
    "print(f'Primeira frase: {example_enc_output[0].shape}')\r\n",
    "print(f'Valor de embedding do primeiro token: {example_enc_output[0][0].shape}')\r\n",
    "\r\n",
    "#----\r\n",
    "print(f'\\nInput batch, shape (batch): {example_input_batch.shape}')\r\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\r\n",
    "print(f'Input batch tokens, shape (batch, s): {example_enc_output.shape}')\r\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "formato do lote de entrada (lote de frases): (64,)\n",
      "algumas frases do lote: [b'Tom est\\xc3\\xa1 anonadado.'\n",
      " b'Quieres que me lave las manos primero, \\xc2\\xbfverdad?']\n",
      "\n",
      "formato do lote de tokens de entrada (lote de tokens): (64, 17)\n",
      "alguns tokens que estão sendo passados: [[ 2 10 20  1  4  3  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      "Saída codificada, shape (batch, s, units): (64, 17, 1024)\n",
      "Estado de saída, shape (batch, units): (64, 1024)\n",
      "\n",
      "Primeira frase: (17, 1024)\n",
      "Valor de embedding do primeiro token: (1024,)\n",
      "\n",
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 17)\n",
      "Input batch tokens, shape (batch, s): (64, 17, 1024)\n",
      "Encoder state, shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Atenção  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# a cabeça da camada de atenção a ser usada aqui é a Bahdanau's additive attention. \r\n",
    "# ref: https://arxiv.org/pdf/1409.0473.pdf\r\n",
    "\r\n",
    "class BahadanauAttention(tf.keras.layers.Layer):\r\n",
    "    def __init__(self, units):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\r\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\r\n",
    "\r\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\r\n",
    "\r\n",
    "    #query vai ser algo gerado pelo decodificador\r\n",
    "    #value é a saída do codificador\r\n",
    "    #maks vai servir para excluir o padding\r\n",
    "    def call(self, query, value, mask):\r\n",
    "        shape_checker = ShapeChecker()\r\n",
    "        shape_checker(query, ('batch', 't', 'query_units')) \r\n",
    "        shape_checker(value, ('batch', 's', 'value_units')) \r\n",
    "        shape_checker(mask, ('batch', 's')) \r\n",
    "\r\n",
    "        w1_query = self.W1(query)\r\n",
    "        shape_checker(w1_query, ('batch', 't', 'attn_units'))\r\n",
    "\r\n",
    "        w2_key = self.W2(value)\r\n",
    "        shape_checker(w2_key, ('batch', 's', 'attn_units'))\r\n",
    "\r\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\r\n",
    "        value_mask = mask\r\n",
    "\r\n",
    "        context_vector, attention_weights = self.attention(\r\n",
    "            inputs = [w1_query, value, w2_key],\r\n",
    "            mask=[query_mask, value_mask],\r\n",
    "            return_attention_scores = True,\r\n",
    "        )\r\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\r\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\r\n",
    "\r\n",
    "        return context_vector, attention_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testando a camada de atenção"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "attention_layer = BahadanauAttention(units)\r\n",
    "print((example_tokens != 0).shape)\r\n",
    "\r\n",
    "#Este é um exemplo de consulta que o decodificador fará\r\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\r\n",
    "\r\n",
    "#\r\n",
    "context_vector, attention_weights = attention_layer(query=example_attention_query, value=example_enc_output, mask=(example_tokens != 0))\r\n",
    "\r\n",
    "print(f'Shape do resultado do vetor de atenção: {context_vector.shape}')\r\n",
    "print(f'Shape dos pesos retornados da camada de atenção: {attention_weights.shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 17)\n",
      "Shape do resultado do vetor de atenção: (64, 2, 1024)\n",
      "Shape dos pesos retornados da camada de atenção: (64, 2, 17)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decodificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#Como são diversos tensores que o decoder recebe e retorna, foi criada classes auxiliares \r\n",
    "class DecoderInput(typing.NamedTuple):\r\n",
    "    new_tokens: Any\r\n",
    "    enc_output: Any\r\n",
    "    mask: Any\r\n",
    "\r\n",
    "class DecoderOutput(typing.NamedTuple):\r\n",
    "    logits: Any\r\n",
    "    attention_weights: Any    \r\n",
    "\r\n",
    "#O decodificador vai receber a saída inteira do codificador para gerar as previsões.\r\n",
    "class Decoder(tf.keras.layers.Layer): \r\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\r\n",
    "        super(Decoder, self).__init__()\r\n",
    "\r\n",
    "        self.dec_units = dec_units\r\n",
    "        self.output_vocab_size = output_vocab_size\r\n",
    "        self.embedding_dim = embedding_dim\r\n",
    "\r\n",
    "        #1º passo: criar camada de Embedding para vetorizar os IDs de tokens\r\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size, embedding_dim)\r\n",
    "\r\n",
    "        #2º passo: a camada GRU para gerar as previsões \r\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\r\n",
    "\r\n",
    "        #3º passo: gerar a camada de atenção para melhorar a previsão. A saída da camada GRU vai servir como query para esta camada\r\n",
    "        self.attention = BahadanauAttention(self.dec_units)\r\n",
    "\r\n",
    "        #4º passo: \r\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh, use_bias=False)\r\n",
    "\r\n",
    "        #5º passo: uma camada que irá produzir previsões logísticas para cada token de saída\r\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\r\n",
    "\r\n",
    "    def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\r\n",
    "        shape_checker = ShapeChecker()\r\n",
    "        shape_checker(inputs.new_tokens, ('batch', 't'))\r\n",
    "        shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\r\n",
    "        shape_checker(inputs.mask, ('batch', 's'))\r\n",
    "        \r\n",
    "        if state is not None:\r\n",
    "          shape_checker(state, ('batch', 'dec_units'))\r\n",
    "        \r\n",
    "        #1º passo. Vetorizando os tokens\r\n",
    "        vectors = self.embedding(inputs.new_tokens)\r\n",
    "        shape_checker(vectors, ('batch', 't', 'embedding_dim'))\r\n",
    "        \r\n",
    "        #2º passo. Processa o vetor de embeddings com a camada GRU\r\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\r\n",
    "        \r\n",
    "        shape_checker(rnn_output, ('batch', 't', 'dec_units'))\r\n",
    "        shape_checker(state, ('batch', 'dec_units'))\r\n",
    "        \r\n",
    "        #3º passo. Usa a saída da camda GRU como query para a camada de atenção\r\n",
    "        context_vector, attention_weights = self.attention( query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\r\n",
    "        \r\n",
    "        shape_checker(context_vector, ('batch', 't', 'dec_units'))\r\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\r\n",
    "        \r\n",
    "        #4º passo. Eqn. (3): Join the context_vector and rnn_output [ct; ht] shape: (batch t, value_units + query_units)\r\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\r\n",
    "        \r\n",
    "        #5º passo. Eqn. (3): `at = tanh(Wc@[ct; ht])`\r\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\r\n",
    "        shape_checker(attention_vector, ('batch', 't', 'dec_units'))\r\n",
    "        \r\n",
    "        #6º passo. Gera as previsões logísticas:\r\n",
    "        logits = self.fc(attention_vector)\r\n",
    "        shape_checker(logits, ('batch', 't', 'output_vocab_size'))\r\n",
    "        \r\n",
    "        return DecoderOutput(logits, attention_weights), state\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testando o decodificador"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Relembrando os dados\r\n",
    "\r\n",
    "print(f'frase: {example_target_batch[1]}')\r\n",
    "print(f'frase vetorizada: {output_text_processor(example_target_batch[1])}')\r\n",
    "\r\n",
    "#O decoder precisa ter o vocabulário para converter os tokens\r\n",
    "#A camada de embedding precisa ter o mesmo tamanho da camada do codificador\r\n",
    "print(f'embedding: {embedding_dim}')\r\n",
    "print(f'unidades: {units}') #Relemebrar\r\n",
    "\r\n",
    "decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\r\n",
    "\r\n",
    "example_output_tokens = output_text_processor(example_target_batch)\r\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\r\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0]) #revisar isso\r\n",
    "\r\n",
    "# Testando o decoder\r\n",
    "dec_result, dec_state = decoder(\r\n",
    "    inputs = DecoderInput(new_tokens=first_token, enc_output=example_enc_output, mask=(example_tokens != 0 )),\r\n",
    "    state = example_enc_state\r\n",
    ")\r\n",
    "\r\n",
    "print(f'\\nshape do cálculo logístico: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\r\n",
    "print(f'shape do estado do decoder: (batch_size, dec_units) {dec_state.shape}')\r\n",
    "\r\n",
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\r\n",
    "\r\n",
    "#decodificando os tokens usando o vocabulário\r\n",
    "vocab = np.array(output_text_processor.get_vocabulary())\r\n",
    "first_word = vocab[sampled_token.numpy()]\r\n",
    "first_word[:5]\r\n",
    "\r\n",
    "print(f'\\nVocabulário: {vocab}')\r\n",
    "print(f'Palavras: {first_word[:5]}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "frase: b\"You want me to wash my hands first, don't you?\"\n",
      "frase vetorizada: [  2   8  37  23   7 858  24 540 203  19  27   8  11   3]\n",
      "embedding: 256\n",
      "unidades: 1024\n",
      "\n",
      "shape do cálculo logístico: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "shape do estado do decoder: (batch_size, dec_units) (64, 1024)\n",
      "\n",
      "Vocabulário: ['' '[UNK]' '[START]' ... 'productive' 'printer' 'principles']\n",
      "Palavras: [['arrow']\n",
      " ['ii']\n",
      " ['bathing']\n",
      " ['proves']\n",
      " ['agent']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Treinamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "#Como o tradutor é uma inteligência mais complexa, necessita de criar as próprias funções que são utilizadas no treinamento\r\n",
    "#No caso desta, precisa da: função de perda, otimizador, atualizador de pesos e o ciclo de treinos (epochs)\r\n",
    "\r\n",
    "#Função de perda\r\n",
    "class MaskedLoss(tf.keras.losses.Loss):\r\n",
    "    def __init__(self):\r\n",
    "        self.name = 'masked_loss'\r\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
    "\r\n",
    "    def __call__(self, y_true, y_pred):\r\n",
    "        shape_checker = ShapeChecker()\r\n",
    "        shape_checker(y_true, ('batch', 't'))\r\n",
    "        shape_checker(y_pred, ('batch', 't', 'logits'))\r\n",
    "\r\n",
    "        # Calcula a perda para cada frase no lote (sequência de frases)\r\n",
    "        loss = self.loss(y_true, y_pred)\r\n",
    "        shape_checker(loss, ('batch', 't'))\r\n",
    "\r\n",
    "        # Mascara as perdas no padding (? revisar)\r\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\r\n",
    "        shape_checker(mask, ('batch', 't')) \r\n",
    "        loss *= mask\r\n",
    "\r\n",
    "        return tf.reduce_sum(loss)\r\n",
    "\r\n",
    "#Classe para treinar o modelo\r\n",
    "class TrainTranslator(tf.keras.Model):\r\n",
    "    def __init__(self, embedding_dim, units, input_text_processor, output_text_processor, use_tf_function=True):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        #Constroi o encoder e o decoder\r\n",
    "        self.encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\r\n",
    "        self.decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\r\n",
    "\r\n",
    "        #copia os dados de entrada e saída\r\n",
    "        self.input_text_processor = input_text_processor\r\n",
    "        self.output_text_processor = output_text_processor\r\n",
    "\r\n",
    "        self.use_tf_function = use_tf_function\r\n",
    "        self.shape_checker = ShapeChecker()\r\n",
    "    \r\n",
    "    def train_step(self, inputs):\r\n",
    "        self.shape_checker = ShapeChecker()\r\n",
    "\r\n",
    "        if self.use_tf_function:\r\n",
    "            return self._tf_train_step(inputs)\r\n",
    "        else:\r\n",
    "            return self._train_step(inputs)\r\n",
    "\r\n",
    "    def _preprocess(self, input_text, target_text):\r\n",
    "        self.shape_checker(input_text, ('batch', ))\r\n",
    "        self.shape_checker(target_text, ('batch', ))\r\n",
    "\r\n",
    "        #Vetorizando o texto\r\n",
    "        input_tokens = self.input_text_processor(input_text)\r\n",
    "        target_tokens = self.output_text_processor(target_text)\r\n",
    "\r\n",
    "        self.shape_checker(input_tokens, ('batch', 's'))\r\n",
    "        self.shape_checker(target_tokens, ('batch', 't'))\r\n",
    "\r\n",
    "        #Converte os IDs para uma mascara\r\n",
    "        input_mask = input_tokens != 0\r\n",
    "        target_mask = target_tokens != 0\r\n",
    "        \r\n",
    "        self.shape_checker(input_mask, ('batch', 's'))\r\n",
    "        self.shape_checker(target_mask, ('batch', 't'))\r\n",
    "\r\n",
    "        return input_tokens, input_mask, target_tokens, target_mask\r\n",
    "\r\n",
    "    def _train_step(self, inputs):\r\n",
    "        input_text, target_text = inputs\r\n",
    "\r\n",
    "        (input_tokens, input_mask, target_tokens, target_mask) = self._preprocess(input_text, target_text)\r\n",
    "        max_target_length = tf.shape(target_tokens)[1]\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            #Codifica a entrada\r\n",
    "            enc_output, enc_state = self.encoder(input_tokens)\r\n",
    "            self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\r\n",
    "            self.shape_checker(enc_state, ('batch', 'enc_units'))\r\n",
    "\r\n",
    "            #Inicializa o estado do decoder com o estado final do encoder\r\n",
    "            #Só irá funcionar se ambos tiverem a mesma quantidade de unidades\r\n",
    "            dec_state = enc_state\r\n",
    "            loss = tf.constant(0.0)\r\n",
    "\r\n",
    "            for t in tf.range(max_target_length - 1):                \r\n",
    "                new_tokens = target_tokens[:, t:t+2]\r\n",
    "                \r\n",
    "                step_loss, dec_state = self._loop_step(new_tokens, input_mask, enc_output, dec_state)\r\n",
    "                loss += step_loss\r\n",
    "\r\n",
    "            #Tira média da perda sobre ...\r\n",
    "            avarage_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\r\n",
    "\r\n",
    "        #Aplica a otimização a cada predição\r\n",
    "        variables = self.trainable_variables \r\n",
    "        gradients = tape.gradient(avarage_loss, variables)\r\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\r\n",
    "        \r\n",
    "        return { 'batch_loss': avarage_loss }\r\n",
    "    \r\n",
    "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\r\n",
    "        input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\r\n",
    "\r\n",
    "        #executa o decoder\r\n",
    "        decoder_input = DecoderInput(new_tokens=input_token, enc_output=enc_output, mask=input_mask)\r\n",
    "        \r\n",
    "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\r\n",
    "\r\n",
    "        self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\r\n",
    "        self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\r\n",
    "        self.shape_checker(dec_state, ('batch', 'dec_units'))\r\n",
    "\r\n",
    "        y = target_token\r\n",
    "        y_pred = dec_result.logits\r\n",
    "        \r\n",
    "        step_loss = self.loss(y, y_pred)\r\n",
    "        return step_loss, dec_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fazendo Treinamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "translator = TrainTranslator(\r\n",
    "    embedding_dim, units,\r\n",
    "    input_text_processor=input_text_processor,\r\n",
    "    output_text_processor=output_text_processor,\r\n",
    "    use_tf_function=False\r\n",
    ")\r\n",
    "\r\n",
    "translator.compile(\r\n",
    "    optimizer=tf.optimizers.Adam(),\r\n",
    "    loss=MaskedLoss(),\r\n",
    ")\r\n",
    "\r\n",
    "print(f'A perda deve começar perto de: {np.log(output_text_processor.vocabulary_size())}')\r\n",
    "\r\n",
    "for n in range(10):\r\n",
    "    print(f'perda: {translator.train_step([example_input_batch, example_target_batch])}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A perda deve começar perto de: 8.517193191416238\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5699353>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5382657>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.475046>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.28627>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.5910664>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.20785>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.5456953>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.3601613>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.1125813>}\n",
      "perda: {'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9514656>}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tcc': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "f16f457e3894a9ee51386b4536b92eca290ee443d329ce528bb63897bfdf32bc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}